## 马尔可夫(Markov)

### **1 马尔可夫性质(Markov Property)**

> 当前时刻的状态仅与前一时刻的状态和动作有关，与其他时刻的状态和动作条件独立

**数学表达式**

> $P[S_t+1|S_t]=P[S_{t+1}|S_1,…..,S_t]$
> 如果在状态$S_t$已知的情况下，状态$S_{t+1}$出现的概率 $P[S_{t+1}|S_t] $和在状态$ S_1,…..,S_t$ 同时发生的情况下，状态$S_{t+1}$出现的概率 $P[S_{t+1}|S_1,…..,S_t]$ 相等。我们称状态$ S_t $具有马尔可夫性质。

**状态转移概率（State Transition Probability）**

> 马尔可夫链处于某时刻 $t$ 的状态 $S$ 在下一时刻 $t+1$ 到达另一状态 $S^′ $的条件概率
> $P_{SS^′}=P[S_{t+1}=S^′|S_t=S]$

**状态转移矩阵**

用矩阵 $P$(State Transition Matrix)来表示从某一状态到另一个状态的概率。 矩阵的**每一行代表从一个状态到所有可能的未来状态的概率**。比如状态 1 表示遇见你，状态 n 表示爱上你，那么 $P_{1n}$ 代表了我遇见你的那一秒爱上你的概率。
$$
P = 
\begin{bmatrix}
p_{11} & \cdots & p_{1n} \\
\vdots & \ddots & \vdots \\
p_{n1} & \cdots & p_{nn}
\end{bmatrix}
$$

### 2 马尔可夫过程(Markov Process (MP))

马尔可夫过程是一个具备了马尔可夫性质的随机过程（random process），是不具备记忆特质的（memoryless）。换言之，马尔可夫过程的条件概率仅仅与系统的当前状态相关，而与它的过去历史或未来状态，都是独立、不相关的。

**马尔可夫过程定义式**

> 马尔可夫过程是一个元组$(tuple)-- <S,P>$
>      $S $是有限的状态
>      $P $是状态转移概率矩阵$P_{SS^′}=P[S_{t+1}=S^′|S_t=S]$

### 3 马尔科夫奖励过程(Markov Reward Process (MPR))

**马尔科夫奖励过程定义式**

> 马尔科夫奖励过程（Markov Reward Process）在马尔科夫过程的基础上增加了奖励$ R$ 和衰减系数$ γ$ ,是一个由 $<S,P,R,γ>$ 组成的元组(tuple):
>     $S$ 是有限的状态
>     $P $是状态转移概率矩阵 $P_{SS^′}=P[S_{t+1}=S^′|S_t=S]$
>     $R$ 是奖励函数 $R_S=\mathbb{E}[R_{t+1}|S_t=s]$
>     $γ $是衰减函数， $γ∈[0,1]$

**累积回报定义式**

> 累积回报 $G_t$ （return）是马尔科夫奖励链上从 t 时刻开始以后未来所有有衰减的奖励（折现的奖励）总和。
> $$
> G_t = R_{T+1}+	\gamma R_{t+2} + \cdots = \textstyle \sum_{k=0}^{ \infty}\gamma ^kR_{t+k+1}
> $$
> $\gamma$ 是衰减系数,取值在$[0,1]$，衰减系数用来调整 $agent$ 对当前和未来奖励的权重。
>
> - 如果 $γ=0 $，说明代理是个短视的人，只在意当前的奖励。
> - 如果 $\gamma = 1$，说明代理对当前和未来汇报一视同仁，有毒。。。
> - 上式中的 $γ$ 一般不是0或者1，而是一个中间值。就是越早赚到钱，对整个累积回报的贡献越大。为啥？因为上式中的 $γ^k$ 在早期的值相对较大，对应的回报 $R_{t+k+1}$占比相对较大。简而言之，”既要着眼于现在，也要放眼于未来“

**价值函数 $V(S) $（Value function）衡量的是某一状态 $S$ 的长期价值**

价值函数定义式

> 马尔科夫奖励过程的某一状态（S）的价值函数V(S)是从这个状态开始的未来期望累计回报：
> $$
> V(s)=\mathbb{E}[G_t | S_t=S]
> $$

### 4 马尔可夫决策过程(Markov decision process (MDP))

马尔可夫决策过程是在马尔可夫奖励过程的基础上加了决策（decision）。换句话说，系统下个体的状态不仅和当前的状态有关，也和当前采取的动作（action）有关。

> 对比：
>
> - 马尔可夫奖励过程 $(MPR)$ 中状态之间的转移概率只和当前所处状态有关。
> - 马尔可夫决策过程 $(MDP)$ 在状态转移中引入了动作（action）的概念，从当前状态到下一个状态发生的概率不仅和当前状态有关，还和所采取的行动有观。

**马尔可夫决策过程定义式**

> 马尔可夫决策过程是在马尔可夫奖励过程的基础上增加了决策（decisions/actions）,是一个由 $<S,A,P,R,\gamma>$ 组成的元组（tuple）：
>
> - $S$ 是有限的状态集 
> - $A$ 是有限的决策/动作集
> - $P$ 是状态转移概率矩阵 $P_{ss^{'}}^{a} = P[S_{t+1}=S^{'}|S_{t}=S, A_{t} = a]  $
> - $R$ 是奖励函数，$R_{s}^{a} = \mathbb{E}[R_{t+1}|S_{t}=s, A_{t} = a] $
> - $\gamma$ 是衰减函数，$\gamma\in[0,1]$

**策略定义式**

一个策略 $\pi$ 是给定状态（states）下，关于所有可能发生行动（actions）的分布:
$$
\pi(a|s)=P[A_t=a | S_t=s]
$$
$\pi(a|s)$表示的是在 t 时刻状态为 s 的情况下，选择执行行为 a 的概率。通过这个公式，策略可以完整的定义一个个体的行为方式。 MDP策略只和当前状态有关（not the history）。这里需要注意的是策略是静态的(stationary)，和时间无关。但是策略可以在未来进行调整。

## 贝尔曼方程(Bellman Equation)

定义式

> 如何简化马尔科夫奖励过程的价值函数呢？我们可以把价值函数分解为两部分：
>
> - 即时奖励$R_{t+1}$
> - 加了权重$\gamma $的后续状态的价值函数 $\gamma v(S_{t+1})$
>
> 则数学表达式为：$v(s)=\mathbb{E}[R_{t+1}+\gamma v(S_{t+1} | S_t=s)]$

换句话说，贝尔曼方程只往前考虑一步。在某个状态下，做了一个行动，得到了立即回报。我就可以将这个立即回报加上未来后续状态的价值函数做为我的总体回报。也就是上式中的 $R_{t+1}+\gamma v(S_{t+1})$ 。对这个总体回报求期望，就得出了状态 $s$ 的价值函数。推导过程如下：
$$
\begin{equation*}
  \begin{aligned}
    v(s)&=\mathbb{E}[G_t|S_t=s] \\
    &=\mathbb{E}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots | S_t=s]\\
    &=\mathbb{E}[R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + \cdots) | S_t=s]\\
    &=\mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t=s]\\
    &=\mathbb{E}[R_{t+1} + \gamma v(S_{t+1}) | S_t=s]
  \end{aligned}
\end{equation*}
$$

## 马尔可夫决策过程寻找最优解

**基于策略$ \pi $的价值函数**可以分为基于策略 $π$ 的**状态价值函数** $v_\pi(s)$ 和基于策略 $\pi$ 的**行为价值函数 $q_π(s,a)$ 

### 状态价值函数定义式

$$
V^\pi(s)=\mathbb{E}_{\pi}[G_t|S_t=s]
$$

基于策略 $π$ 的状态价值函数 $v_π(s)$，即可以表示特定状态下某个策略的价值，也可以表示特定策略下某状态的价值

### 动作价值函数定义式

$$
Q^{\pi}(s,a)=\mathbb{E}_{\pi}[G_t|S_t = s,A_t=a]
$$

基于策略$π$的行为价值函数 $q_π(s,a)$ 表示对当前状态$s$执行某一特定行为 $a$ 以后再遵循当前策略 $π $所获得的未来期望收益

#### 状态价值函数和动作价值函数之间的关系

在使用$\pi$策略中，状态$s$的价值等于在该状态下基于策略$\pi$采取所有动作的概率与相应的价值相乘再求和的结果，表达式为：
$$
V^{\pi}(s) = \sum_{a\in A}\pi(a|s)Q^{\pi}(s,a)
$$
使用策略$\pi$时，状态$s$下采取动作$a$的价值等于即时奖励加上经过衰减后的所有可能的下一个状态的状态转移概率与相应的价值的乘积：
$$
Q^{\pi}(s,a)=R(s,a)+\gamma \sum_{s^{'}\in S}P(s^{'}|s,a)V^{\pi}(s^{'})
$$

## 贝尔曼期望方程(Bellman Expectation Equation)

两个价值函数的贝尔曼期望方程：
$$
\begin{equation*}
  \begin{aligned}
			V^\pi(s)&=\mathbb{E}_{\pi}[R_t + \gamma V^{\pi}(S_{t+1})|S_t=s]\\
			&=\sum_{a\in A}\pi(a|s)(R(s,a)+\gamma \sum_{s^{'}\in S}P(s^{'}|s,a)V^{\pi}(s^{'}))
  \end{aligned}
\end{equation*}
$$

$$
\begin{equation*}
  \begin{aligned}
			Q^{\pi}(s,a)&=\mathbb{E}_{\pi}[R_t + \gamma Q^{\pi}(S_{t+1},A_{t+1})|S_t=s,A_t=a]\\
			&=R(s,a)+\gamma \sum_{s^{'}\in S}P(s^{'}|s,a)\sum_{a\in A}\pi(a^{'}|s^{'})Q^{\pi}(s^{'},a^{'}))
  \end{aligned}
\end{equation*}
$$

## 部分可观察马尔科夫决策过程基本模型

> $MDP$ 模型假设环境状态完全可观察，但很多实际问题并非如此。比如机器人不可能准确观察环境状态的每个部分，环境中多多少少会存在隐藏信息，比如被障碍物遮挡住的部分、传感器感知范围以外的部分等。部分可观察马尔科夫决策过程（$POMDP$）引入**观察和观察模型**把 $MDP$ 扩展到部分可观察环境。观察是跟环境状态相关的随机变量，表示智能体对环境状态的测量值，观察的具体取值由环境的观察模型决定。

定义：部分可观察马尔科夫决策过程形 式上可以被定义成一个六元组 $⟨S, A, O, T, Ω, R⟩$, 其中：

- $S$ 是状态空间$（State Space）$，即所有可能环境状态的集合
- $A $是动作空间$（Action Space）$，即智能体所有可选行动的集合
- $T : S × A × S → [0, 1] $是状态转移函数$（Transition Function）$，$T(s ′ | s, a) = Pr(s ′ | s, a) $给出状态 $s $下执行动作 $a$ 后，系统转移到状态 $s ′ $的概率
- $R : S × A → R$ 是回报函数$（Reward Function）$，$R(s, a) $给出智能体在状态 $ s$ 下执行动作 $a$ 后从环境立即获得的回报值（又称奖赏值）
- $O$ 是观察空间$（Observation Space）$，即智能体所有可能获得的观察集合； 
- $Ω : S × A × O → [0, 1]$ 是观察函数$（Observation Function）$，$Ω(o | s, a)$ 给 出智能体执行完行动 $a$，环境转移到状态 $s$ 后，观测到观察 $ o$ 的概率



$POMDP $ 可以被转化称定义在信念状态空间上的 $MDP$。**信念状态指智能体对目前环境所处状态的概率估计，是所有过去观察、行动历史的统计充分量**。记信念状态为 $b$，则 $b(s)$ 给出当前环境处于状态 $s$ 的概率，即 $b(s) = Pr(s | b)$。给 定初始信念状态 $b_0 $和观察—行动历史$ h = (a_0, o_1, a_1, o_2, . . . a_{t−1}, o_t)$，则当前 的信念状态可以递归地根据贝叶斯推理方法唯一确定，即：
$$
b'(s') = \eta \Omega(o|s',a) \sum_{s\in S}T(s'|s,a)b(s)
$$
其中，$\eta = \frac{1}{P(o|b,a)}$ 为正则化因子，展开式为：
$$
\eta = \frac{1}{\sum_{s\in S'}\Omega(o|s',a)\sum_{s\in S}T(s'|s,a)b(s)}
$$
公式(9)便完成了信念状态的更新，即$b' =  \varsigma(b,a,o)$，$ \varsigma$ 函数一般称为 **贝叶斯滤波器**

​		令 $B$ 为所有信念状态的集合——即信念空间，则 $POMDP$ 的策略 $π$ 定义成 信念空间到动作空间的映射，即 $π : B → A$。跟 $MDP $类似，求解 $POMDP$ 的目标 就是找到最优策略 $π ∗$ 使得期望累积回报最大。引入信念状态以后，$POMDP $ 被转化称定义在信念状态上的 $MDP$，又称为 **贝叶斯自适应MDP**：$⟨B, A, T +, r⟩$，其中 $B$ 是$ BAMDP$ 的状态空间，$A$ 是行动空间， $r(b, a) = ∑ s∈S b(s)R(s, a)$ 是立即立即回报函数，$T +$ 是状态转移函数，定义为：
$$
T^+(b′| b, a) = ∑_{o∈O}1[b′ = ζ(b, a, o)]Ω(o | b, a)
$$
其中$ 1$ 指示函数（Indicator Function）。$BAMDP$ 的贝尔曼最优性等式是：
$$
V^∗(b) = max_{a∈A} (  r(b, a) + γ∑_{o∈O}Ω(o | b, a)V∗(ζ(b, a, o))  )
$$
给定最优函数值 $V^*$ ，最优策略$\pi^*$ 可以得到如下式子：
$$
π^∗(b) = \underset{a \in A}{argmax}(r(b, a) + γ∑_{o∈O}Ω(o | b, a)V∗(ζ(b, a, o)))
$$
$BAMDP$ 的最优策略，加以正确的信念状态更新就是原 $POMDP$ 问题的最优 策略。剩下的问题就是如何求解 $BAMDP$，但这并不容易，因为 $BAMDP$ 是定义 在信念空间上的连续 $MDP$，下一节将利用 $BAMDP$ 值函数所具有的特殊性质来 求解这一问题。给定一个初始信念状态，信念状态和历史是可以互换使用。
