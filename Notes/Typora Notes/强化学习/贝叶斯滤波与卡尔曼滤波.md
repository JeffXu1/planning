# 贝叶斯滤波

# 卡尔曼滤波器

## 1 数学基础

### 1.1 递归算法

测量数据为 $z_k$，$\hat x_t$ 为估计数据，现在我们通过测量的均值来估计真是数据
$$
\begin{equation*}
  \begin{aligned}
    \hat x_k&=\frac {1}{k}(z_1+z_2+z_3+\cdots + z_k)\\
    &=\frac{1}{k}(z_1+z_2+z_3+\cdots + z_k)+\frac{1}{k} z_k\\
    &=\frac{1}{k} \frac{k-1}{k-1} (z_1+z_2+z_3+\cdots + z_{k-1})+\frac{1}{k} z_k\\
    &=\frac{k-1}{k}\hat x_{k-1}+\frac{1}{k}z_k\\
    &=\hat x_{k-1}-\frac{1}{k}\hat x_{k-1}+\frac{1}{k}z_k
   
  \end{aligned}
\end{equation*}\\
$$
故可以得到：$\hat x_k = \hat x_{k-1} +\frac {1}{k} (z_k-\hat x_{k-1})$，这便是递归(Recursive)的思想

------

归纳可以得到：
$$
\hat x_k = \hat x_{k-1} +K_k (z_k-\hat x_{k-1})\\
当前的估计值 = 上次的估计值 + 系数 * （当前的测量值 - 上一次的估计值）
$$
这里的系数 $K_k$ 即为卡尔曼增益(Kalman Gain)，这表明卡尔曼滤波器可以只关注上一时刻的值而不用回去关注很久以前的值，数学表达式为：
$$
K_k=\frac{e_{EST_{k-1}}}{e_{EST_{k-1}} + e_{MEA_{k}}} \\
$$
其中，估计误差：$e_{EST}$，测量误差(传感器精度)：$e_{MEA}$

分析上式：

- $e_{EST}$ >> $e_{MEA}$时，$K_k\to 1，\hat x_k = \hat x_{k-1} + (z_k-\hat x_{k-1})=z_k$，说明测量误差很小，系统更相信测量值

- $e_{EST}$ << $e_{MEA}$时，$K_k \to 0，\hat x_k = \hat x_{k-1} $，说明估计误差很小，系统更相信上一次的估计值

### 1.2 数据融合

> 可以理解为将一个传感器数据作为估计值，另一个传感器数据作为测量值，两者做卡尔曼滤波

**案例**：现有两个电子秤来称重量，假设二者都满足正太分布，A称测量值$z_1=30g$，标准差$\sigma_1=2g$；B称测量值$z_2=32g$，标准差$\sigma_2=4g$，那么如何根据这两组数据来估计真实值 $\hat z$ 呢？

------

利用卡尔曼滤波的思想有：$\hat z= z_1 + k(z_2-z_1),k\in[0,1]$，那么要求得一个$k$使得 $\sigma_z$最小。

有：
$$
\begin{equation*}
  \begin{aligned}
    \sigma^2_z &= var(z_1+k(z_2-z_1))\\
    &=var((1-k)z_1+kz_2) \ \ (z_1,z_2二者相互独立)\\
    &=var((1-k)z_1)+var(kz_2)\\
    &=(1-k)^2var(z_1)+k^2var(z_2)\\
    &=(1-k)^2\sigma_1^2+k^2\sigma^2_2
  \end{aligned}
\end{equation*}\\
$$
对上式求极值：
$$
\frac{d\sigma_z^2}{dk} = 0 \implies k=\frac{\sigma_1^2}{\sigma_1^2+\sigma_2^2} = \frac{2^2}{2^2+4^2}=0.2\\
$$
那么最终估计值 $\hat z= z_1 + k(z_2-z_1) = 30 + 0.2*(32-30)=30.4$，$\sigma_z^2 = 3.2$

### 1.3 协方差矩阵

> 将方差和协方差在一个矩阵中表现出来，体现变量间的联动关系

#### 协方差

通常，在提到协方差的时候，需要对其进一步区分。（1）随机变量的协方差。跟数学期望、方差一样，是分布的一个总体参数。（2）样本的协方差。是样本集的一个统计量，可作为联合分布总体参数的一个估计。在实际中计算的通常是样本的协方差。

##### 随机变量的协方差

在概率论和统计中，协方差是对两个随机变量**联合分布**线性相关程度的一种度量。两个随机变量越线性相关，协方差越大，完全线性无关，协方差为零。定义如下。
$$
cov(X,Y)=E[(X−E[X])(Y−E[Y])]
$$
当$X，Y$是同一个随机变量时，XX与其自身的协方差就是XX的方差，可以说方差是协方差的一个特例
$$
cov(X,X)=E[(X−E[X])(X−E[X])]
$$
或
$$
var(X)=cov(X,X)=E[(X−E[X])^2]
$$
由于随机变量的取值范围不同，两个协方差不具备可比性。如$X，Y，Z$分别是三个随机变量，想要比较$X$与$Y$的线性相关程度强，还是$X$与$Z$的线性相关程度强，通过$cov(X,Y) $与$cov(X,Z)$ 无法直接比较。定义相关系数$η$为
$$
η=\frac{cov(X,Y)}{\sqrt{var(X)⋅var(Y)}}
$$
通过$X$的方差$var(X)$ 与$Y$的方差$var(Y)$对协方差$cov(X,Y)$归一化，得到相关系数$η$，$η$的取值范围是[−1,1][−1,1]。$1$表示完全线性相关，$−1$表示完全线性负相关，$0$表示线性无关。线性无关并不代表完全无关，更不代表相互独立。

#### 协方差矩阵

##### 多维随机变量的协方差矩阵

对多维随机变量$X=[X1,X2,X3,...,Xn]^T$，我们往往需要计算各维度两两之间的协方差，这样各协方差组成了一个$n×n$ 的矩阵，称为协方差矩阵。协方差矩阵是个对称矩阵，对角线上的元素是各维度上随机变量的方差。我们定义协方差矩阵为$Σ$，这个符号与求和$∑$相同，需要根据上下文区分。矩阵内的元素$Σij$为：
$$
\Sigma_{ij}=\operatorname{cov}(X_i,X_j)=\operatorname{E}\big[(X_i-\operatorname{E}[X_i])(X_j-\operatorname{E}[X_j])\big]
$$
这样这个矩阵为：
$$
\Sigma=\operatorname{E}\big[(\textbf X-\operatorname{E}[\textbf X]\big)(\textbf X-\operatorname{E}[\textbf X])^T]\\
=\begin{bmatrix} 
    \operatorname{cov}(X_1, X_1) &
    \operatorname{cov}(X_1, X_2) &
    \cdots &
    \operatorname{cov}(X_1, X_n) \\
    \operatorname{cov}(X_2, X_1) &
    \operatorname{cov}(X_2, X_2) &
    \cdots &
    \operatorname{cov}(X_2, X_n) \\
    \vdots &
    \vdots &
    \ddots &
    \vdots \\
    \operatorname{cov}(X_n, X_1) &
    \operatorname{cov}(X_n, X_2) &
    \cdots &
    \operatorname{cov}(X_n, X_n) 
\end{bmatrix}\\
=\begin{bmatrix} 
    \operatorname{E}\big[(X_1-\operatorname{E}[X_1])(X_1-\operatorname{E}[X_1])\big] &                                           \operatorname{E}\big[(X_1-\operatorname{E}[X_1])(X_2-\operatorname{E}[X_2])\big] &
    \cdots &
    \operatorname{E}\big[(X_1-\operatorname{E}[X_1])(X_n-\operatorname{E}[X_n])\big] \\
    \operatorname{E}\big[(X_2-\operatorname{E}[X_2])(X_1-\operatorname{E}[X_1])\big] &
    \operatorname{E}\big[(X_2-\operatorname{E}[X_2])(X_2-\operatorname{E}[X_2])\big] &
    \cdots &
    \operatorname{E}\big[(X_2-\operatorname{E}[X_2])(X_n-\operatorname{E}[X_n])\big] \\
    \vdots &
    \vdots &
    \ddots &
    \vdots \\
    \operatorname{E}\big[(X_n-\operatorname{E}[X_n])(X_1-\operatorname{E}[X_1])\big] &
    \operatorname{E}\big[(X_n-\operatorname{E}[X_n])(X_2-\operatorname{E}[X_2])\big] &
    \cdots &
    \operatorname{E}\big[(X_n-\operatorname{E}[X_n])(X_n-\operatorname{E}[X_n])\big] &
\end{bmatrix}
$$

### 1.4 状态空间方程

$$
状态变量：&x_k = Ax_{k-1}+Bu_k+w_{k-1}  \ \ (w_{k-1} 为过程噪音)\\
测量值：&z_k=x_k+v_k \ \ (v_k为测量噪音)
$$

## 2 卡尔曼增益推导

略

## 3 误差协方差矩阵

$$
x_k = A(x_k - \hat x_{k-1}) ++Bu_{k-1} + w_{k-1} \\
z_k= Hx_k + v_{k} \\
其中：p(w)\thicksim N(0,Q), p(v) \thicksim N(0,R)
$$

先验估计：
$$
\hat x^-_k =A\hat x_{k-1} + Bu_{k-1}
$$
后验估计：
$$
\hat x_{k} = \hat x^{-}_k +k_k (z_k-H \hat x^{-}_k)
$$
卡尔曼增益：

> $e_k= x_k-\hat x_k$ ，$p(e_k) \thicksim N(0,p)$
>
> 则 $p=E[ee^T]$ 化简可得到：$p_k = p^-_k - k_kHp^-_k - p^-_kH^Tk^T_k + k_kHp^-_kH^Tk_K^T + k_KRk_k^T$
>
> 对 $p_k$ 求极值，可以得到 $k_k$，所以卡尔曼增益是一个求最优解得到的结果

$$
k_k = \frac{p^-_kH^T}{Hp^-_kH^T+R}
$$

如何求解$p^-_k$ ？

因为 $e_k = x_k - \hat x_k$，且 $p(e_k) \thicksim N(0,p)$，则有：
$$
\begin{equation*}
  \begin{aligned}
    p^-_k &= E(e_ke_k^T)\\
    &=E[(Ae_{k-1}+w_{k-1})(Ae_{k-1}+w_{k-1})^T] \\
    &=AE[e_{k-1}e^T_{k-1}]A^T + E[w_{k-1}w^T_{k-1}] \\
    &=Ap_{k-1}A^T + Q
  \end{aligned}
\end{equation*}\\
$$
在从卡尔曼增益的推导过程中得到 $p_k$ 的表达式：
$$
\begin{equation*}
  \begin{aligned}
      p_k &= p^-_k - k_kHp^-_k - p^-_kH^Tk^T_k + k_kHp^-_kH^Tk_K^T + k_KRk_k^T \\
      &=p^-_k -k_kHp^-_K \\ \\
      \Longrightarrow p_k &= (I - k_kH)p^-_k
  \end{aligned}
\end{equation*}
$$

------

## 4 卡尔曼滤波器工作流程

> 卡尔曼滤波器针对的是**线性系统**

### 状态方程 & 观测器
$$
x_k = Ax_{k-1}+bu_{k-1}+w_{k-1}\\
z_k = H_{k-1} + v_k \\
$$

其中：$p(w)\thicksim N(0,Q), p(v) \thicksim N(0,R)$

### **预测步**

1. 先验：
   $$
   \hat x^-_k =A\hat x_{k-1} + Bu_{k-1}
   $$

2. 先验误差协方差：
   $$
   p_k^- ==Ap_{k-1}A^T + Q
   $$

### **校正步**

1. 卡尔曼增益：
   $$
   k_k = \frac{p^-_kH^T}{Hp^-_kH^T+R}
   $$

2. 后验估计：
   $$
   \hat x_{k} = \hat x^{-}_k +k_k (z_k-H \hat x^{-}_k)
   $$

3. 更新误差协方差：
   $$
   p_k = (I - k_kH)p^-_k
   $$
   

------



# 扩展卡尔曼滤波器

> Extended Kalman Filter (EKF)

## 线性系统(Linear System)

$$
x_k = Ax_{k-1}+bu_{k-1}+w_{k-1}\\
z_k = H_{k-1} + v_k \\
其中：p(w)\thicksim N(0,Q), p(v) \thicksim N(0,R)
$$

预测步骤(先验)：
$$
\hat x^-_k =A\hat x_{k-1} + Bu_{k-1} \\
\hat p^-_k= AP_{k-1}A^T + Q
$$
校正步骤(后验)：
$$
k_k = \frac{p^-_kH^T}{Hp^-_kH^T+R} \\
\hat x_k = \hat x^-_k + k_k(z_k-H\hat x^-_k) \\
\hat p_k= （I - K_kH)p^-_k
$$

## 非线性系统(Non Linear System)

$$
x_k= f(x_{k-1},u_{k-1},w_{k-1}) \\
z_k = h(x_k,v_k) \\
其中，f(),h()为非线性系统，p(w)\thicksim N(0,Q), p(v) \thicksim N(0,R)
$$

上述非线性系统问题在于，正太分布的随机变量通过非线性系统后就不再是正太分布。

### 线性化(Linearization)

可以通过一阶泰勒展开将非线性系统线性化 $f(x)= f(x_0)+ \frac{\partial f}{\partial x}(x-x_0)$ ，又因为系统有误差，所以不可能得到真实值，退而求其次，可以选择在后验估计 $\hat x_{k-1}$ 处线性化，
$$
x_k=f(\hat x_{k-1},u_{k-1},0) + A(x_k - \hat x_{k-1}) + w_{w_{k-1}} \\
$$
其中：

$A=\frac{\partial f}{\partial x}|\hat x_{k-1},u_{k-1}$，$W_k=\frac{\partial f}{\partial w}|\hat x_{k-1},u_{k-1}$，两个均为雅克比矩阵

由于误差事先未知，所以假设为$w_{k-1} = 0$，则有$f(\hat x_{k-1},u_{k-1},0) =\widetilde {x}_k$
$$
z_k= h(\widetilde x_K,v_k) + H(x_k - \widetilde x_k) + V_{v_{k}}
$$
其中：

$H=\frac{\partial H}{\partial x}|\widetilde x_k$，$v_k=\frac{\partial f}{\partial v}|\widetilde x_k$，两个均为雅克比矩阵

同样由于误差事先未知，所以假设为$v_k = 0$，则有$h(\widetilde x_k,0) =\widetilde z_k$

### 扩展卡尔曼滤波器完整步骤

#### 状态方程 & 观测器

$$
x_k=\hat x_k + A(x_k - \hat x_{k-1}) + w_{w_{k-1}} \\
z_k= \hat z_k + H(x_k - \hat x_k) + v_{v_{k}}
$$

其中：$p(w)\thicksim N(0,Q), p(w_w)\thicksim N(0,wQw^T) ,p(v_{v_{k}})\thicksim N(0,vRv^T)$

#### **预测步**

1. 先验：
   $$
   \hat x^-_k =f(\hat x_{k-1},u_{k-1},0)
   $$

2. 先验误差协方差：
   $$
   \hat p^-_k= AP_{K-1}A^T + wQw^T
   $$

#### **校正步**

1. 卡尔曼增益：
   $$
   k_k = \frac{p^-_kH^T}{Hp^-_{k}H^T+vRv^T}
   $$

2. 后验估计：
   $$
   \hat x_k = \hat x^-_k + k_k(z_k-h(\hat x^-_k,0))
   $$

3. 更新误差协方差：
   $$
   \hat p_k= （I -k_kH)p^-_k
   $$

# 无迹卡尔曼滤波

> Unscented Kalman Filter(UKF)

参考：https://blog.shipengx.com/archives/cfd8b171.html
